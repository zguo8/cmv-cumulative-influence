{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skipCRF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qb5iX9kULm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "import pystruct\n",
        "from pystruct.models import ChainCRF\n",
        "from pystruct.models import GraphCRF\n",
        "from pystruct.learners import FrankWolfeSSVM\n",
        "\n",
        "import re\n",
        "import datetime\n",
        "\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "from tensorflow import Graph\n",
        "\n",
        "import scipy.stats\n",
        "\n",
        "import string\n",
        "  \n",
        "import itertools\n",
        "from itertools import chain\n",
        "\n",
        "import scipy.stats\n",
        "\n",
        "import spicy \n",
        "from scipy import spatial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDQl7kkx8OzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# cmv_threads column index (starting from 0)\n",
        "TH_ID_INDEX = 0\n",
        "TH_TITLE_INDEX = 1\n",
        "TH_TEXT_INDEX = 4\n",
        "TH_AUTHOR_INDEX = 5\n",
        "TH_LENGTH_INDEX = 6\n",
        "\n",
        "# cmv_comments column index (starting from 0)\n",
        "THREAD_ID_INDEX = 0\n",
        "CONTENT_INDEX = 8 # text content\n",
        "AUTHOR_ID_INDEX = 5\n",
        "DELTA_EARN_INDEX = 9\n",
        "FLAIR_INDEX = 6\n",
        "SENTIMENT_INDEX = 16\n",
        "COMMENT_ID_INDEX = 2\n",
        "REPLY_TO_INDEX = 3\n",
        "\n",
        "\n",
        "HEDGE_INDEX = 23\n",
        "ASSERTIVES_INDEX = 17\n",
        "FACTIVES_INDEX =  18\n",
        "IMPLICATIVES_INDEX = 19 \n",
        "REPORT_VERBS_INDEX = 20\n",
        "OPINION_NEG_INDEX = 21\n",
        "OPINION_POS_INDEX = 22\n",
        "STRONG_SUBJ_INDEX = 24\n",
        "WEAK_SUBJ_INDEX = 25\n",
        "\n",
        "# running example\n",
        "EXAMPLE = '3j895i'\n",
        "EXAMPLE_LEN = 5\n",
        "EXAMPLE_INDEX = 4\n",
        "\n",
        "# parameters\n",
        "# TODO - grid search\n",
        "THRESHOLD_COS = 0.5\n",
        "TFIDF_MAX_FEATURES = 100\n",
        "\n",
        "HAS_DELTA_ONLY = False\n",
        "TRAIN_THREAD_NUM = 900\n",
        "\n",
        "FEED_PAIRED = False\n",
        "\n",
        "# variables\n",
        "data_set = []\n",
        "data_label = []\n",
        "\n",
        "train_set = []\n",
        "train_label = []\n",
        "\n",
        "test_set = []\n",
        "test_label = []\n",
        "\n",
        "pickle_overwrite = False\n",
        "\n",
        "prefix_pkl = \"data_cmv/pkl/new_skip_thread_\"\n",
        "\n",
        "file_dataset = prefix_pkl + \"dataset.pkl\"\n",
        "file_model = prefix_pkl + \"crf.pkl\"\n",
        "file_lp_prob = prefix_pkl + \"result_prob.pkl\"\n",
        "\n",
        "hedges = []\n",
        "with open('data_cmv/input/hedge_list.txt', 'r') as f:\n",
        "    content = f.readlines()\n",
        "    hedges = [x.strip() for x in content]\n",
        "\n",
        "def form_lexicon(filename):\n",
        "    word_list = []\n",
        "    with open(filename, 'r', encoding='mac_roman', newline='') as f:\n",
        "        content = f.readlines()\n",
        "        for row in content:\n",
        "            if len(row.strip())==0 or row[0][0] == \"#\":\n",
        "                continue\n",
        "            word_list.append(row.strip())\n",
        "    return word_list\n",
        "            \n",
        "hedges = form_lexicon('data_cmv/input/hedge_list.txt')\n",
        "\n",
        "assertives = form_lexicon('data_cmv/input/assertives_hooper1975.txt')\n",
        "\n",
        "factives = form_lexicon('data_cmv/input/factives_hooper1975.txt')\n",
        "\n",
        "implicatives = form_lexicon('data_cmv/input/implicatives_karttunen1971.txt')\n",
        "\n",
        "opinion_neg = form_lexicon('data_cmv/input/opinion_negative-words.txt')\n",
        "\n",
        "opinion_pos = form_lexicon('data_cmv/input/opinion_positive-words.txt')\n",
        "\n",
        "report_verbs = form_lexicon('data_cmv/input/report_verbs.txt')\n",
        "\n",
        "\n",
        "\n",
        "def print_time():\n",
        "    print(datetime.datetime.now())\n",
        "\n",
        "print_time()\n",
        "\n",
        "\n",
        "def pickle_save(filename, data2pkl):\n",
        "    global pickle_overwrite\n",
        "    if pickle_overwrite == True:\n",
        "        fileObject = open(filename,'wb') \n",
        "        pickle.dump(data2pkl,fileObject)\n",
        "        fileObject.close()\n",
        "        \n",
        "\n",
        "        \n",
        "def pickle_load(filename, data2pkl):\n",
        "    fileObject = open(filename,'wb') \n",
        "    pickle.dump(data2pkl,fileObject)\n",
        "    fileObject.close()\n",
        "        \n",
        "# ------------------------------------------------------------------\n",
        "# Extract features\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]\n",
        "\n",
        "\n",
        "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, seq):\n",
        "        column = np.recarray(shape=(len(seq),),\n",
        "                               dtype=[('seq', object), ('content', object)])\n",
        "\n",
        "        column['seq'] = seq\n",
        "        seq_array = np.array(seq)\n",
        "\n",
        "        column['content'] = seq_array[:, CONTENT_INDEX]\n",
        "\n",
        "        return column\n",
        "\n",
        "\n",
        "class ProduceStatFeatureDict(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, cos):\n",
        "        self.cos = cos\n",
        "    \n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, seq):\n",
        "        # get recent posts from the opinion holder\n",
        "        recent_posts = []\n",
        "        cur_thread = threads_dict[seq[0][THREAD_ID_INDEX]]\n",
        "\n",
        "        features_list = []\n",
        "        for i, entry in enumerate(seq):\n",
        "            word_cnt_entry = len(re.split(r'[^0-9A-Za-z]+',entry[CONTENT_INDEX]))\n",
        "            word_token = re.split(r'[^0-9A-Za-z]+',entry[CONTENT_INDEX].lower())\n",
        "\n",
        "            if FEED_PAIRED:\n",
        "                simi_init = self.cos[0][i+1]\n",
        "            else:\n",
        "                simi_init = self.cos[0][i+1]\n",
        "\n",
        "            feature_dict = {\n",
        "            # ---------------- linguistic features ----------------\n",
        "            'num_words': word_cnt_entry,\n",
        "            'length_comment': len(entry[CONTENT_INDEX]),\n",
        "            'num_sentences': entry[CONTENT_INDEX].count('.'), \n",
        "            'hedge': int(entry[HEDGE_INDEX])*1.0/word_cnt_entry,\n",
        "              # from the 2013 ACL - Bias paper\n",
        "            'assertives': int(entry[ASSERTIVES_INDEX])*1.0/word_cnt_entry,\n",
        "            'factives': int(entry[FACTIVES_INDEX])*1.0/word_cnt_entry,\n",
        "            'implicatives': int(entry[IMPLICATIVES_INDEX])*1.0/word_cnt_entry,\n",
        "            'report_verbs': int(entry[REPORT_VERBS_INDEX])*1.0/word_cnt_entry,\n",
        "            'opinion_neg': int(entry[OPINION_NEG_INDEX])*1.0/word_cnt_entry,\n",
        "            'opinion_pos': int(entry[OPINION_POS_INDEX])*1.0/word_cnt_entry, \n",
        "            'strong_subj': int(entry[STRONG_SUBJ_INDEX])*1.0/word_cnt_entry,\n",
        "            'weak_subj': int(entry[WEAK_SUBJ_INDEX])*1.0/word_cnt_entry,\n",
        "             # word category features from [Tan2016WWW]\n",
        "            'definite_articles': entry[CONTENT_INDEX].lower().count('the')*1.0/word_cnt_entry,\n",
        "            'indefinite_articles': sum(entry[CONTENT_INDEX].lower().count(x) for x in (\"a\", \"an\"))*1.0/word_cnt_entry,        \n",
        "            \"1st_pron\": FirstPersonPronUsage(word_token)*1.0/word_cnt_entry,\n",
        "            \"2nd_pron\": SecondPersonPronUsage(word_token)*1.0/word_cnt_entry, \n",
        "            'sentiment': float(re.sub('[^0-9]', '', entry[SENTIMENT_INDEX])) if (\n",
        "                re.sub('[^0-9]', '', entry[SENTIMENT_INDEX]).strip()) else 0,\n",
        "            'num_questions': entry[CONTENT_INDEX].count('?'),   \n",
        "            'examples':  sum(entry[CONTENT_INDEX].lower().count(x) for x in (\"for example\", \"for instance\", \"e.g\", \"eg\")),\n",
        "            'links': 1 if \"http\" in entry[CONTENT_INDEX] else 0,\n",
        "\n",
        "            # ---------------- context-based features ----------------\n",
        "            'order': i + 1,\n",
        "            'length_discussion': len(seq),\n",
        "            'relative_order': (i + 1) * 1.0 / len(seq),\n",
        "            'user_flair': int(re.sub('[^0-9]', '', entry[FLAIR_INDEX])) if (\n",
        "                re.sub('[^0-9]', '', entry[FLAIR_INDEX]).strip()) else 0,\n",
        "            'authentication': 1 if entry[AUTHOR_ID_INDEX] == cur_thread[TH_AUTHOR_INDEX] else 0,\n",
        "            # ---------------- interaction-based features ----------------\n",
        "            'simi_initial':simi_init,\n",
        "            # similarity with the recent opinion holder's post\n",
        "            'simi_recent': cos[int(entry[COMMENT_ORDER_INDEX])].T[0:holder_post_cnt].mean() if holder_post_cnt > 0 \\\n",
        "                and i >= holder_post_cnt \n",
        "                else cos[len(seq)][int(entry[COMMENT_ORDER_INDEX])],      \n",
        "                    'simi_recent': simi_recent,\n",
        "            'direct_response': (comment_id_list.index(entry[REPLY_TO_INDEX])+1.0)/len(seq) if entry[REPLY_TO_INDEX] in comment_id_list\n",
        "                else 0,\n",
        "            'quotation': 1 if \"&gt;\" in entry[CONTENT_INDEX] else 0  \n",
        "            }\n",
        "            features_list.append(feature_dict)\n",
        "  \n",
        "        return features_list\n",
        "\n",
        "\n",
        "def HedgeUsage(text):\n",
        "    return sum(map(text.lower().count, hedges))\n",
        "\n",
        "\n",
        "def FirstPersonPronUsage(text_token):\n",
        "    pron_1st = [\"i\", \"me\", \"we\", \"us\", \"my\", \"mine\"]\n",
        "#     return sum(map(text.lower().count, pron_1st))\n",
        "    return sum(el in pron_1st for el in text_token )\n",
        "\n",
        "\n",
        "def SecondPersonPronUsage(text_token):\n",
        "    pron_2nd = [\"you\", \"your\", \"yours\"]\n",
        "#     return sum(map(text.lower().count, pron_2nd))\n",
        "    return sum(el in pron_2nd for el in text_token )\n",
        "\n",
        "\n",
        "def SecondPersonPronUsageInSent(sent_token):\n",
        "    pron_2nd = [\"you\", \"your\", \"yours\"]\n",
        "    cnt_sent = 0\n",
        "    for sent in sent_token:\n",
        "        if sum(map(sent.count, pron_2nd)) > 0:\n",
        "            cnt_sent += 1\n",
        "    return cnt_sent\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "# for each thread, form a feature vector\n",
        "def form_features(seq,cos):\n",
        "    features = []\n",
        "    feature_unioned = Pipeline([\n",
        "        ('extractColumns', ColumnExtractor()),\n",
        "        # Use FeatureUnion to combine the features from subject and body\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list=[\n",
        "                ('seq_stats', Pipeline([\n",
        "                    ('select', ItemSelector(key='seq')),\n",
        "                    ('stats', ProduceStatFeatureDict(cos=cos)),  # returns a list of dicts\n",
        "                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
        "                ])),\n",
        "            ],\n",
        "            transformer_weights={\n",
        "#                 'content_similarity': 0.8,\n",
        "                'seq_stats': 1,\n",
        "            },\n",
        "        )\n",
        "         )\n",
        "    ])\n",
        "\n",
        "    output = feature_unioned.fit_transform(seq)\n",
        "\n",
        "    return output.toarray()\n",
        "\n",
        "\n",
        "def form_edges(seq, cos):\n",
        "    edges_linear = np.vstack([np.arange(len(seq)-1), np.arange(1, len(seq))])\n",
        "    edges_skip = []\n",
        "    #edges_skip = np.array([[0, 2], [1, 2]])\n",
        "\n",
        "    docs = []\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # option 1: if similarity between two comments > threshold\n",
        "    for i in range(1, len(seq)):\n",
        "        for j in range(i+2, len(seq)):\n",
        "            # if cosine similarity > THRESHOLD_COS, form an edge\n",
        "            # print(cos[i,j], i, j)            \n",
        "            # form skip edges using TF-IDF cos similarity           \n",
        "            if cos[i,j] > THRESHOLD_COS:\n",
        "                edges_skip.append([i-1, j-1])\n",
        "            # form skip edges using direct response\n",
        "            elif seq[j][REPLY_TO_INDEX] == seq[i][COMMENT_ID_INDEX]:\n",
        "#                 print(\"direct response\")\n",
        "                edges_skip.append([i-1, j-1]) \n",
        "            elif seq[j][AUTHOR_ID_INDEX] == seq[i][AUTHOR_ID_INDEX]:\n",
        "#                 print(\"direct response\")\n",
        "                edges_skip.append([i-1, j-1]) \n",
        "        \n",
        "    edges_skip = np.array(edges_skip)\n",
        "    \n",
        "    if len(edges_skip) > 0:\n",
        "        edges = np.vstack([edges_linear.T, edges_skip]).T\n",
        "    else:\n",
        "        edges = edges_linear\n",
        "    return edges\n",
        "\n",
        "def form_labels(seq):\n",
        "    labels = []\n",
        "    for entry in seq:\n",
        "        if entry[DELTA_EARN_INDEX] == \"1\":\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "\n",
        "def calculate_similarity(vec1,vec2):\n",
        "    simi = 1 - spatial.distance.cosine(vec1, vec2) \n",
        "    return simi\n",
        "    \n",
        "\n",
        "def sent_encode(text):\n",
        "    return nltk.sent_tokenize(text)\n",
        "\n",
        "\n",
        "# def USE_simi(docs,cur_thread,thread_op_encoded):\n",
        "def USE_simi(seq,docs,cur_thread):\n",
        "    sentences = []\n",
        "    encoded_sentences = []\n",
        "    thread_author = cur_thread[TH_AUTHOR_INDEX]    \n",
        "\n",
        "    # encode each comments, including texts from opinion holder. docs could be seq or subseq or paired\n",
        "    for i,para in enumerate(docs):\n",
        "        if para == \"\":\n",
        "            encoded_sentences.append(np.zeros(len(encoded_sentences[0])))\n",
        "        else:\n",
        "            sent_token = nltk.sent_tokenize(para)\n",
        "            sentences.append(sent_token)\n",
        "\n",
        "            encoded_sentences.append(encoder.encode(sent_token))\n",
        "        \n",
        "    if len(encoded_sentences) == 0:\n",
        "        print(\"Ah!! empty docs\")\n",
        "\n",
        "\n",
        "    cos_simi = np.zeros((len(encoded_sentences),len(encoded_sentences)))\n",
        "\n",
        "    if FEED_PAIRED:\n",
        "        # only need to calculate cos with op\n",
        "        comm_sent_i = encoded_sentences[0]\n",
        "        for j in range(1,len(encoded_sentences)):\n",
        "            comm_sent_j = encoded_sentences[j]\n",
        "            max_simi = 0\n",
        "            for x,y in itertools.product(comm_sent_i,comm_sent_j):\n",
        "                simi = calculate_similarity(x,y)\n",
        "                if simi>max_simi and simi != 1:\n",
        "                    max_simi = simi                \n",
        "            cos_simi[0][j] = max_simi\n",
        "\n",
        "    else:\n",
        "        # calculate cos_simi for every pair of seq[], and fetch later\n",
        "        for i,comm_sent_i in enumerate(encoded_sentences):\n",
        "            for j in range(i,len(encoded_sentences)):\n",
        "                comm_sent_j = encoded_sentences[j]\n",
        "                # in pairwise, compare sentences in comment i and comment j,\n",
        "                # calculate max(similarity)            \n",
        "                if i==j:\n",
        "                    cos_simi[i][j] = 1\n",
        "                elif seq[i-1][AUTHOR_ID_INDEX] != thread_author and seq[j-1][AUTHOR_ID_INDEX] != thread_author:\n",
        "                    cos_simi[i][j] = 0\n",
        "                    cos_simi[j][i] = 0\n",
        "                else:\n",
        "                    max_simi = 0\n",
        "                    for x,y in itertools.product(comm_sent_i,comm_sent_j):\n",
        "                        simi = calculate_similarity(x,y)\n",
        "                        if simi>max_simi and simi != 1:\n",
        "                            max_simi = simi                \n",
        "                    cos_simi[i][j] = max_simi\n",
        "                    cos_simi[j][i] = max_simi\n",
        "            \n",
        "    return cos_simi  \n",
        "    \n",
        "\n",
        "cnt_thread = 0 \n",
        "\n",
        "def ProcessThread(thread_seq):\n",
        "    global cnt_thread, encoder\n",
        "    cur_thread = threads_dict[thread_seq[0][THREAD_ID_INDEX]]\n",
        "    thread_author = cur_thread[TH_AUTHOR_INDEX]\n",
        "    \n",
        "    # calculate similarity between comments based on the whole thread\n",
        "    seq_array = np.array(thread_seq)\n",
        "#     print(seq_array)\n",
        "    docs = seq_array[:, CONTENT_INDEX]\n",
        "    docs = docs.tolist()    \n",
        "   \n",
        "    # add the initial post to the end of seq\n",
        "    docs.append(cur_thread[TH_TEXT_INDEX])\n",
        "    \n",
        "    # create a 2D numpy array as cos\n",
        "    cos_len = len(thread_seq)+1 # appended initial statement to thread\n",
        "    cos = np.zeros(shape=(cos_len,cos_len))    \n",
        "    \n",
        "     # ----------------------------------------------------------------    \n",
        "    # option 1 - calculate cos with tfidf, cos[0][0] ... cos[n][n]\n",
        "    # --------------------------------\n",
        "#     vect = TfidfVectorizer()\n",
        "#     tfidf = vect.fit_transform(docs)\n",
        "#     trunc = TruncatedSVD(tfidf)\n",
        "#     cos = (tfidf * tfidf.T).A\n",
        "\n",
        "    # --------------------------------\n",
        "    # option 2 - calculate cos with ELMO embeddings\n",
        "\n",
        "#     cos =Elmo4Docs(docs)\n",
        "    # --------------------------------\n",
        "    # option 3 - calculate cos with USE  \n",
        "#     emb = encoder.encode(docs)\n",
        "    \n",
        "#     for i,vec1 in enumerate(emb):\n",
        "#         for j,vec2 in enumerate(emb):\n",
        "#             cos[i,j] = dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
        "         \n",
        "    # --------------------------------\n",
        "    # option 4 - calculate cos with USE at sentence level and calculate the max(similarity)\n",
        "    \n",
        "    # add the initial post to the beginning of each seq or subseq\n",
        "    thread_op = cur_thread[TH_TITLE_INDEX]+\". \"+cur_thread[TH_TEXT_INDEX] if len(cur_thread[TH_TEXT_INDEX]) > 0  else cur_thread[TH_TITLE_INDEX]\n",
        "    seq_array = np.array(thread_seq)\n",
        "    docs = [thread_op] + seq_array[:, CONTENT_INDEX].tolist()\n",
        "    \n",
        "    simi = np.zeros((len(docs),len(docs)))\n",
        "    simi = USE_simi(thread_seq,docs,cur_thread)\n",
        "    # --------------------------------\n",
        "    \n",
        "    # print(\"--------------------- thread ---------------------\")\n",
        "    if len(thread_seq) < 3:\n",
        "        print(thread_seq[0][0] + \" - less than 2 comments\")\n",
        "    else:\n",
        "        edges = form_edges(thread_seq, simi)\n",
        "        features = form_features(thread_seq, simi)\n",
        "        labels = form_labels(thread_seq)\n",
        "\n",
        "        if len(features[0])< 26:\n",
        "            print(thread_seq[0][0] + \" - less features\")\n",
        "        else:\n",
        "#             features_norm = preprocessing.normalize(features, norm='l2')\n",
        "            data_set.append((features, edges))\n",
        "            data_label.append(np.array(labels))\n",
        "#             print(thread_seq[0][0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGEYMXwgDQ92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using tensorflow USE for word embeddings\n",
        "\n",
        "USE_URL = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n",
        "cachedir = 'c6f5954ffa065cdb2f2e604e740e8838bf21a2d3'\n",
        "# cachedir = 'test1'\n",
        "\n",
        "class USE(object):\n",
        "    def __init__(self, model_url=USE_URL):\n",
        "        if \"TFHUB_CACHE_DIR\" in os.environ:\n",
        "            tfdata = os.environ['TFHUB_CACHE_DIR']\n",
        "            model_url= tfdata + \"/\" + cachedir\n",
        "#             model_url = tfdata\n",
        "            print(tfdata)\n",
        "        else:\n",
        "            print(\"TFHUB_CACHE_DIR=None\")\n",
        "\n",
        "        graph = Graph()\n",
        "        with graph.as_default():\n",
        "            embed = hub.Module(model_url)\n",
        "            self.sentences = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "            self.encoded_text = tf.cast(embed(self.sentences), tf.float32)\n",
        "            init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        graph.finalize()\n",
        "\n",
        "        self.session = tf.Session(graph=graph)\n",
        "        self.session.run(init_op)\n",
        "\n",
        "    def encode(self, sentences):\n",
        "        return self.session.run(self.encoded_text, feed_dict={self.sentences: sentences})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONyQ8cM3K7c9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# initiate an encoder\n",
        "encoder = USE()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjrEN1wS8kjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# main entrance of the project\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "threads_reader = csv.reader(open(\"data_cmv/cmv_threads.csv\", newline='', encoding='mac_roman'), delimiter=',',\n",
        "                       quotechar='\"')\n",
        "\n",
        "                            \n",
        "threads_dict = {}\n",
        "examined_threads = {}\n",
        "\n",
        "    \n",
        "try:\n",
        "    for row in threads_reader:\n",
        "        if row[0] != \"thread_id\":\n",
        "            threads_dict[row[0]] = row\n",
        "            examined_threads[row[0]] = 0\n",
        "        \n",
        "except Exception as ex:\n",
        "    sys.stderr.write('Exception\\n')\n",
        "    extype, exvalue, extrace = sys.exc_info()\n",
        "    traceback.print_exception(extype, exvalue, extrace)\n",
        "    \n",
        "print_time()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnkbLF6h8lgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "spamreader = csv.reader(open(\"data_cmv/cmv_comments.csv\", newline='', encoding='mac_roman'), delimiter=',', quotechar='\"')\n",
        "\n",
        "prev_thread = \"\"\n",
        "thread_seq = []\n",
        "\n",
        "\n",
        "    \n",
        "try:\n",
        "    row_cnt = 0\n",
        "    print(\"processing thread...\", datetime.datetime.now())\n",
        "    for row in spamreader:\n",
        "\n",
        "        if row[0] == prev_thread:\n",
        "            thread_seq.append(row)\n",
        "        elif prev_thread != \"\" and prev_thread != \"thread_id\":\n",
        "            ProcessThread(thread_seq)\n",
        "\n",
        "            thread_seq = []\n",
        "            thread_seq.append(row)\n",
        "        row_cnt += 1    \n",
        "        if row_cnt % 10000 == 0:\n",
        "            print(row_cnt, datetime.datetime.now())\n",
        "          \n",
        "        prev_thread = row[0]\n",
        "    # for the list thread\n",
        "    ProcessThread(thread_seq)\n",
        "    print(\"Done processing.\")\n",
        "\n",
        "except Exception as ex:\n",
        "    sys.stderr.write('Exception\\n')\n",
        "    extype, exvalue, extrace = sys.exc_info()\n",
        "    traceback.print_exception(extype, exvalue, extrace)\n",
        "    #sys.exc_clear()\n",
        "    \n",
        "print(datetime.datetime.now())\n",
        "\n",
        "print(len(data_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gireEc8H8pUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_THREAD_NUM = 5000\n",
        "# split the dataset into train and test\n",
        "train_set = data_set[:TRAIN_THREAD_NUM]\n",
        "train_label = data_label[:TRAIN_THREAD_NUM]\n",
        "\n",
        "test_set = data_set[TRAIN_THREAD_NUM:]\n",
        "test_label = data_label[TRAIN_THREAD_NUM:]\n",
        "\n",
        "print(\"train: \",len(train_set), \"test: \", len(test_set))\n",
        "print(\"total: \", len(data_set))\n",
        "\n",
        "\n",
        "# pickle dataset\n",
        "\n",
        "train_test_dict = {\n",
        "    \"train_set\":train_set,\n",
        "    \"train_label\":train_label,\n",
        "    \"test_set\":test_set,\n",
        "    \"test_label\":test_label\n",
        "}\n",
        "\n",
        "if pickle_overwrite:\n",
        "    fileObject = open(file_dataset,'wb') \n",
        "    pickle.dump(train_test_dict,fileObject)\n",
        "    fileObject.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SMQ17fg8se5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the CRF model into a SVM classifier\n",
        "# model = GraphCRF(directed=True, inference_method=\"max-product\")\n",
        "# model = GraphCRF(directed=True, inference_method=\"ad3\")\n",
        "\n",
        "print(datetime.datetime.now())\n",
        "\n",
        "model = GraphCRF(directed=True, inference_method = ('lp', {'relaxed' : True}))\n",
        "\n",
        "# Use a n-slack SSVM learner\n",
        "# ssvm = FrankWolfeSSVM(model=model, C=.1, max_iter=50)\n",
        "# predict_result = ssvm.predict(test_set)\n",
        "# ssvm.fit(train_set, train_label)\n",
        "# score = ssvm.score(test_set, test_label)\n",
        "\n",
        "from pystruct.learners import OneSlackSSVM\n",
        "learner = OneSlackSSVM(model=model, C=.02, max_iter=10)\n",
        " \n",
        "learner.fit(train_set, train_label)\n",
        "\n",
        "lp_probs = learner.predict(test_set)\n",
        "\n",
        "# print(lp_probs[0][0][:,1])\n",
        "print(datetime.datetime.now())\n",
        "\n",
        "# pickle trained crf\n",
        "if pickle_overwrite:\n",
        "    fileObject = open(file_model,'wb') \n",
        "    pickle.dump(learner,fileObject)\n",
        "    fileObject.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf6gS5FI8wzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx = []\n",
        "# new_xx = []\n",
        "for x in lp_probs:\n",
        "#     new_x = []\n",
        "    xx.append(x[0][:,1])\n",
        "    \n",
        "lp_probs_trans = np.concatenate(xx)\n",
        "test_label_trans = np.concatenate(test_label)\n",
        "\n",
        "print(len(lp_probs_trans),len(test_label_trans))\n",
        "\n",
        "\n",
        "# pickle predicted results\n",
        "fileObject = open(file_lp_prob,'wb') \n",
        "pickle.dump(lp_probs_trans,fileObject)\n",
        "fileObject.close()\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score, accuracy_score, jaccard_similarity_score, roc_curve, roc_auc_score\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.utils.fixes import signature\n",
        "\n",
        "# ----------------  plot AUC-ROC ----------------\n",
        "\n",
        "print('ROC-AUC: ', roc_auc_score(test_label_trans, lp_probs_trans))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(test_label_trans, lp_probs_trans, pos_label=1)\n",
        "\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 20})\n",
        "f=plt.figure()\n",
        "plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()\n",
        "f.savefig(\"data_cmv/figures/exp-skip-thread-auc-large-6.pdf\", bbox_inches='tight')\n",
        "\n",
        "# ----------------  plot PRC ----------------\n",
        "\n",
        "\n",
        "average_precision = average_precision_score(test_label_trans, lp_probs_trans)\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(test_label_trans, lp_probs_trans)\n",
        "\n",
        "pos_rate = sum(test_label_trans)/len(test_label_trans)\n",
        "\n",
        "f=plt.figure()\n",
        "matplotlib.rcParams.update({'font.size': 20})\n",
        "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.step(recall, precision, color='b', alpha=0.2,\n",
        "         where='post', label='AP = %0.2f' % average_precision)\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "plt.plot([0, 1], [pos_rate, pos_rate], 'r--')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "# plt.title('PRC for skip-chain CRF with threads \\n AP={0:0.2f}'.format(average_precision))\n",
        "f.savefig(\"data_cmv/figures/exp-skip-thread-prc-large-6.pdf\", bbox_inches='tight')\n",
        "\n",
        "print(precision)\n",
        "print(recall)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZIwvl0uIiTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# measure MRR (mean reciprocal rank)\n",
        "\n",
        "print(\"discussion total #\", len(test_label))\n",
        "ranked_results = []\n",
        "\n",
        "multi_delta_cnt = 0\n",
        "for i in range(len(xx)):\n",
        "    list1 = test_label[i]\n",
        "    list2 = xx[i]\n",
        "\n",
        "    list2_ranked, list1_ranked = zip(*sorted(zip(list2,list1), reverse=True))\n",
        "    list1_ranked_int = list(map(int, list1_ranked))\n",
        "    # optional: ignore non-delta sub sequences\n",
        "    if sum(list1_ranked_int) > 1:\n",
        "        multi_delta_cnt += 1\n",
        "    ranked_results.append(list1_ranked_int)\n",
        "\n",
        "print(\"discussion for mrr #\", len(ranked_results))\n",
        "\n",
        "rs = (np.asarray(r).nonzero()[0] for r in ranked_results)\n",
        "\n",
        "mrr = np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
        "\n",
        "print(mrr)\n",
        "\n",
        "print(multi_delta_cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}