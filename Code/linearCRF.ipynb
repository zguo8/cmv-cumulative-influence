{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linearCRF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbKD6EooafcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import sys\n",
        "import traceback\n",
        "import pystruct\n",
        "from pystruct.models import ChainCRF\n",
        "from pystruct.models import GraphCRF\n",
        "from pystruct.learners import FrankWolfeSSVM\n",
        "import re\n",
        "import datetime\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "from sklearn import preprocessing\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "from tensorflow import Graph\n",
        "import scipy.stats\n",
        "import string\n",
        "import itertools\n",
        "from itertools import chain\n",
        "import spicy\n",
        "from scipy import spatial\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score, accuracy_score, jaccard_similarity_score, roc_curve, roc_auc_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import average_precision_score,precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.fixes import signature\n",
        "\n",
        "\n",
        "import nltk, string\n",
        "# plt.style.use('ggplot')\n",
        "import sklearn_crfsuite\n",
        "\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics as crf_metrics\n",
        "from sklearn.manifold import TSNE\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pISUEdIa8Az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# cmv_threads column index (starting from 0)\n",
        "TH_ID_INDEX = 0\n",
        "TH_TITLE_INDEX = 1\n",
        "TH_TEXT_INDEX = 4\n",
        "TH_AUTHOR_INDEX = 5\n",
        "TH_LENGTH_INDEX = 6\n",
        "\n",
        "# cmv_comments column index (starting from 0)\n",
        "THREAD_ID_INDEX = 0\n",
        "COMMENT_ID_INDEX = 2\n",
        "REPLY_TO_INDEX = 3\n",
        "AUTHOR_ID_INDEX = 5\n",
        "CONTENT_INDEX = 8  # text content\n",
        "DELTA_EARN_INDEX = 9\n",
        "FLAIR_INDEX = 6\n",
        "SENTIMENT_INDEX = 16\n",
        "COMMENT_ORDER_INDEX = 1\n",
        "\n",
        "HEDGE_INDEX = 23\n",
        "ASSERTIVES_INDEX = 17\n",
        "FACTIVES_INDEX =  18\n",
        "IMPLICATIVES_INDEX = 19 \n",
        "REPORT_VERBS_INDEX = 20\n",
        "OPINION_NEG_INDEX = 21\n",
        "OPINION_POS_INDEX = 22\n",
        "STRONG_SUBJ_INDEX = 24\n",
        "WEAK_SUBJ_INDEX = 25\n",
        "\n",
        "# parameters\n",
        "THRESHOLD_COS = 0.5\n",
        "TFIDF_MAX_FEATURES = 100\n",
        "\n",
        "HAS_DELTA_ONLY = False\n",
        "\n",
        "FEATURE_NUM = 29 #57\n",
        "\n",
        "pickle_overwrite = False\n",
        "\n",
        "FEED_PAIRED = False\n",
        "\n",
        "test_name = \"linear_thread\"\n",
        "\n",
        "prefix_pkl = \"data_cmv/pkl/\"\n",
        "\n",
        "file_raw_data = prefix_pkl + test_name + \"_raw\"\n",
        "file_dataset = prefix_pkl + test_name + \"_dataset\"\n",
        "file_model = prefix_pkl + test_name + \"_crf\"\n",
        "file_lp_prob = prefix_pkl + test_name + \"_result_prob\"\n",
        "file_encoded_data = prefix_pkl + test_name + \"_encoded_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryAC1d0t9nK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variables\n",
        "data_set = []\n",
        "data_label = []\n",
        "\n",
        "train_set = []\n",
        "train_label = []\n",
        "\n",
        "test_set = []\n",
        "test_label = []\n",
        "\n",
        "delta_record_cnt = 0\n",
        "nondelta_record_cnt = 0\n",
        "\n",
        "cnt_ignored_seq = 0\n",
        "cnt_thread = 0\n",
        "cnt_train_subseq = 0\n",
        "cnt_test_subseq = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NXpFeSaacOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using tensorflow USE for word embeddings\n",
        "\n",
        "USE_URL = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n",
        "cachedir = 'c6f5954ffa065cdb2f2e604e740e8838bf21a2d3'\n",
        "\n",
        "class USE(object):\n",
        "    def __init__(self, model_url=USE_URL):\n",
        "        if \"TFHUB_CACHE_DIR\" in os.environ:\n",
        "            tfdata = os.environ['TFHUB_CACHE_DIR']\n",
        "            model_url= tfdata + \"/\" + cachedir\n",
        "#             model_url = tfdata\n",
        "            print(tfdata)\n",
        "        else:\n",
        "            print(\"TFHUB_CACHE_DIR=None\")\n",
        "\n",
        "        graph = Graph()\n",
        "        with graph.as_default():\n",
        "            embed = hub.Module(model_url)\n",
        "            self.sentences = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "            self.encoded_text = tf.cast(embed(self.sentences), tf.float32)\n",
        "            init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        graph.finalize()\n",
        "\n",
        "        self.session = tf.Session(graph=graph)\n",
        "        self.session.run(init_op)\n",
        "\n",
        "    def encode(self, sentences):\n",
        "        return self.session.run(self.encoded_text, feed_dict={self.sentences: sentences})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRms0NEGbnvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def form_lexicon(filename):\n",
        "    word_list = []\n",
        "    with open(filename, 'r', encoding='mac_roman', newline='') as f:\n",
        "        content = f.readlines()\n",
        "        for row in content:\n",
        "            if len(row.strip())==0 or row[0][0] == \"#\":\n",
        "                continue\n",
        "            word_list.append(row.strip())\n",
        "    return word_list\n",
        "            \n",
        "hedges = form_lexicon('data_cmv/input/hedge_list.txt')\n",
        "\n",
        "assertives = form_lexicon('data_cmv/input/assertives_hooper1975.txt')\n",
        "\n",
        "factives = form_lexicon('data_cmv/input/factives_hooper1975.txt')\n",
        "\n",
        "implicatives = form_lexicon('data_cmv/input/implicatives_karttunen1971.txt')\n",
        "\n",
        "opinion_neg = form_lexicon('data_cmv/input/opinion_negative-words.txt')\n",
        "\n",
        "opinion_pos = form_lexicon('data_cmv/input/opinion_positive-words.txt')\n",
        "\n",
        "report_verbs = form_lexicon('data_cmv/input/report_verbs.txt')\n",
        "\n",
        "\n",
        "def print_time(*args):\n",
        "    text = \"\"\n",
        "    for arg in args:\n",
        "        text += arg\n",
        "    print(\"[\",datetime.datetime.now(),\"]\", text)\n",
        "\n",
        "\n",
        "def curtime():    \n",
        "    currentDT = datetime.datetime.now()\n",
        "    return currentDT.strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def pickle_save(filename, data2pkl):\n",
        "    global pickle_overwrite\n",
        "    if pickle_overwrite == True:\n",
        "        fileObject = open(filename,'wb') \n",
        "        pickle.dump(data2pkl,fileObject)\n",
        "        fileObject.close()\n",
        "\n",
        "        \n",
        "def pickle_load(filename):\n",
        "    fileObject = open(filename,'rb') \n",
        "    data_unpkl = pickle.load(fileObject)\n",
        "    fileObject.close()\n",
        "    return data_unpkl\n",
        "\n",
        "\n",
        "print_time(\"Program begins.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HucCs8jDcOx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# main entrance of the project\n",
        "# ------------------------------------------------------------------\n",
        "threads_reader = csv.reader(open(\"data_cmv/cmv_threads.csv\", newline='', encoding='mac_roman'), delimiter=',',\n",
        "                        quotechar='\"')\n",
        "\n",
        "\n",
        "threads_dict = {}\n",
        "    \n",
        "try:\n",
        "    for row in threads_reader:\n",
        "        if row[0] != \"thread_id\":\n",
        "            threads_dict[row[0]] = row\n",
        "        \n",
        "except Exception as ex:\n",
        "    sys.stderr.write('Exception\\n')\n",
        "    extype, exvalue, extrace = sys.exc_info()\n",
        "    traceback.print_exception(extype, exvalue, extrace)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWuAP3SIcWjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# # Extract features\n",
        "# import spacy\n",
        "# nlp = spacy.load('en')\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]\n",
        "\n",
        "\n",
        "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, seq):\n",
        "        column = np.recarray(shape=(len(seq),),\n",
        "                             dtype=[('seq', object), ('content', object)])\n",
        "\n",
        "        column['seq'] = seq\n",
        "        seq_array = np.array(seq)\n",
        "\n",
        "        column['content'] = seq_array[:, CONTENT_INDEX]\n",
        "        return column\n",
        "\n",
        "def RemovePuncMap():\n",
        "    return dict((ord(char), None) for char in string.punctuation)\n",
        "\n",
        "\n",
        "def SimilarityBetween(text1, text2):   \n",
        "    # nltk.download('punkt') # if necessary...\n",
        "#     stemmer = nltk.stem.porter.PorterStemmer()\n",
        "    vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
        "    return cosine_sim(text1, text2, vectorizer)\n",
        "\n",
        "def SimilarityWithRecentPosts(entry, recent_posts):\n",
        "    simi = []\n",
        "    for post in recent_posts:\n",
        "        simi.append(SimilarityBetween(entry, post))\n",
        "    \n",
        "    return sum(l)/len(l) if len(l) != 0 else 0\n",
        "    \n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    stemmer = nltk.stem.porter.PorterStemmer()\n",
        "    return [stemmer.stem(item) for item in tokens]\n",
        "\n",
        "'''remove punctuation, lowercase, stem'''\n",
        "def normalize(text):\n",
        "    return stem_tokens(nltk.word_tokenize(text.lower().translate(RemovePuncMap())))\n",
        "\n",
        "\n",
        "def cosine_sim(text1, text2, vectorizer):\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]\n",
        "\n",
        "\n",
        "def HedgeUsage(text):\n",
        "    return sum(map(text.lower().count, hedges))\n",
        "\n",
        "\n",
        "def FirstPersonPronUsage(text_token):\n",
        "    pron_1st = [\"i\", \"me\", \"we\", \"us\", \"my\", \"mine\"]\n",
        "#     return sum(map(text.lower().count, pron_1st))\n",
        "    return sum(el in pron_1st for el in text_token )\n",
        "\n",
        "\n",
        "def SecondPersonPronUsage(text_token):\n",
        "    pron_2nd = [\"you\", \"your\", \"yours\"]\n",
        "#     return sum(map(text.lower().count, pron_2nd))\n",
        "    return sum(el in pron_2nd for el in text_token )\n",
        "\n",
        "\n",
        "def SecondPersonPronUsageInSent(sent_token):\n",
        "    pron_2nd = [\"you\", \"your\", \"yours\"]\n",
        "    cnt_sent = 0\n",
        "    for sent in sent_token:\n",
        "        if sum(map(sent.count, pron_2nd)) > 0:\n",
        "            cnt_sent += 1\n",
        "    return cnt_sent\n",
        "\n",
        "\n",
        "def RecentSimiCommentSenti(entry, oh_reply_dict):\n",
        "    if len(oh_reply_dict) == 0:\n",
        "        return 0,0\n",
        "    \n",
        "    d = {}\n",
        "   \n",
        "    for key, value in oh_reply_dict.items():\n",
        "        if value[2] < entry[COMMENT_ORDER_INDEX]:\n",
        "            d[key] = value\n",
        "        \n",
        "    if len(d) == 0:\n",
        "        return 0,0\n",
        "    \n",
        "    arr = sorted(d.items(), key=lambda x: x[1][1])\n",
        "\n",
        "    return pair_simi(entry[CONTENT_INDEX], arr[0][0]), pair_simi(entry[CONTENT_INDEX],arr[len(arr)-1][0])\n",
        "\n",
        "\n",
        "def pair_simi(com_1, com_2):\n",
        "    if com_1 == \"\" or com_2 == \"\":\n",
        "        return 0\n",
        "    \n",
        "    token_1 = encoder.encode(nltk.sent_tokenize(com_1))\n",
        "    token_2 = encoder.encode(nltk.sent_tokenize(com_2))\n",
        "    \n",
        "    max_simi = 0\n",
        "    for x,y in itertools.product(token_1,token_2):\n",
        "        simi = calculate_similarity(x,y)\n",
        "        if simi > max_simi and simi != 1:\n",
        "            max_simi = simi     \n",
        "            \n",
        "    return max_simi\n",
        "    \n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "\n",
        "def form_labels(seq):\n",
        "    labels = []\n",
        "    for entry in seq:\n",
        "        if entry[DELTA_EARN_INDEX] == \"1\":\n",
        "            labels.append('1')\n",
        "        else:\n",
        "            labels.append('0')\n",
        "\n",
        "    return labels\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sdzOS-BgGxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def form_edges(seq, cos):\n",
        "    edges_linear = np.vstack([np.arange(len(seq)-1), np.arange(1, len(seq))])\n",
        "    edges_skip = []\n",
        "    #edges_skip = np.array([[0, 2], [1, 2]])\n",
        "\n",
        "    docs = []\n",
        "    # ---------------------------------------------------\n",
        "    # option 1: only if similarity between two comments > threshold\n",
        "    for i in range(1, len(seq)):\n",
        "        for j in range(i+2, len(seq)):\n",
        "            # if cosine similarity > THRESHOLD_COS, form an edge\n",
        "            # print(cos[i,j], i, j)            \n",
        "            # form skip edges using TF-IDF cos similarity           \n",
        "            if cos[i,j] > THRESHOLD_COS:\n",
        "                edges_skip.append([i-1, j-1])\n",
        "#             # form skip edges using direct response\n",
        "#             elif seq[j][REPLY_TO_INDEX] == seq[i][COMMENT_ID_INDEX]:\n",
        "# #                 print(\"direct response\")\n",
        "#                 edges_skip.append([i-1, j-1]) \n",
        "#             elif seq[j][AUTHOR_ID_INDEX] == seq[i][AUTHOR_ID_INDEX]:\n",
        "# #                 print(\"same author\")\n",
        "#                 edges_skip.append([i-1, j-1]) \n",
        "\n",
        "    # ---------------------------------------------------\n",
        "        \n",
        "    edges_skip = np.array(edges_skip)\n",
        "    \n",
        "    if len(edges_skip) > 0:\n",
        "        edges = np.vstack([edges_linear.T, edges_skip]).T\n",
        "    else:\n",
        "        edges = edges_linear\n",
        "    # print(edges)\n",
        "    return edges.T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daG_RHnxcbnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# for linear CRF thread structure\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "# for each thread, form a feature vector\n",
        "def form_feature_dicts(seq, sent_seq, cos, oh_reply_dict):\n",
        "    recent_posts = []\n",
        "    cur_thread = threads_dict[seq[0][THREAD_ID_INDEX]]\n",
        "\n",
        "    # get recent posts from the opinion holder\n",
        "    holder_post_cnt = 0\n",
        "    comment_id_list = []\n",
        "    for entry in seq:\n",
        "        comment_id_list.append(entry[COMMENT_ID_INDEX])\n",
        "        if entry[AUTHOR_ID_INDEX] == cur_thread[TH_AUTHOR_INDEX]:\n",
        "            recent_posts.append(entry[CONTENT_INDEX])\n",
        "            holder_post_cnt += 1\n",
        "\n",
        "    features_list = []\n",
        "    for i, entry in enumerate(seq):\n",
        "        word_cnt_entry = len(re.split(r'[^0-9A-Za-z]+',entry[CONTENT_INDEX]))\n",
        "        word_token = re.split(r'[^0-9A-Za-z]+',entry[CONTENT_INDEX].lower())\n",
        "        sent_token = sent_seq[i+1]     \n",
        "        \n",
        "        most_neg_simi, most_pos_simi = RecentSimiCommentSenti(entry, oh_reply_dict)\n",
        "        \n",
        "        if FEED_PAIRED:\n",
        "            simi_init = cos[0][i+1]\n",
        "            simi_recent = 0\n",
        "        else:\n",
        "            simi_init = cos[0][i+1]\n",
        "            simi_recent = cos[i+1].T[0:holder_post_cnt].mean() if holder_post_cnt > 0 and i >= holder_post_cnt else 0    \n",
        "        \n",
        "        feature_dict = {\n",
        "         # ---------------- linguistic features ----------------\n",
        "        'num_words': word_cnt_entry,\n",
        "        'length_comment': len(entry[CONTENT_INDEX]),\n",
        "        'num_sentences': entry[CONTENT_INDEX].count('.'),  \n",
        "        'hedge': int(entry[HEDGE_INDEX])*1.0/word_cnt_entry,\n",
        "          # from the 2013 ACL - Bias paper\n",
        "        'assertives': int(entry[ASSERTIVES_INDEX])*1.0/word_cnt_entry,\n",
        "        'factives': int(entry[FACTIVES_INDEX])*1.0/word_cnt_entry,\n",
        "        'implicatives': int(entry[IMPLICATIVES_INDEX])*1.0/word_cnt_entry,\n",
        "        'report_verbs': int(entry[REPORT_VERBS_INDEX])*1.0/word_cnt_entry,\n",
        "        'opinion_neg': int(entry[OPINION_NEG_INDEX])*1.0/word_cnt_entry,\n",
        "        'opinion_pos': int(entry[OPINION_POS_INDEX])*1.0/word_cnt_entry, \n",
        "        'strong_subj': int(entry[STRONG_SUBJ_INDEX])*1.0/word_cnt_entry,\n",
        "        'weak_subj': int(entry[WEAK_SUBJ_INDEX])*1.0/word_cnt_entry,\n",
        "         # word category features from [Tan2016WWW]\n",
        "        'definite_articles': entry[CONTENT_INDEX].lower().count('the')*1.0/word_cnt_entry,\n",
        "        'indefinite_articles': sum(entry[CONTENT_INDEX].lower().count(x) for x in (\"a\", \"an\"))*1.0/word_cnt_entry,        \n",
        "        \"1st_pron\": FirstPersonPronUsage(word_token)*1.0/word_cnt_entry,\n",
        "        \"2nd_pron\": SecondPersonPronUsage(word_token)*1.0/word_cnt_entry,\n",
        "        \"2nd_pron_sent\":  SecondPersonPronUsageInSent(sent_token)*1.0/len(sent_token) if len(sent_token) > 0 else 0,\n",
        "        'sentiment': float(re.sub('[^0-9]', '', entry[SENTIMENT_INDEX])) if (\n",
        "            re.sub('[^0-9]', '', entry[SENTIMENT_INDEX]).strip()) else 0,\n",
        "        'num_questions': entry[CONTENT_INDEX].count('?'),   \n",
        "        'examples':  sum(entry[CONTENT_INDEX].lower().count(x) for x in (\"for example\", \"for instance\", \"e.g\", \"eg\")),\n",
        "        'links': 1 if \"http\" in entry[CONTENT_INDEX] else 0,\n",
        "\n",
        "        # ---------------- context-based features ----------------\n",
        "        'order_in_discussion': (int(entry[COMMENT_ORDER_INDEX])+1.0)/int(cur_thread[TH_LENGTH_INDEX]),\n",
        "        'sub_order': i + 1,\n",
        "        'relative_order': (i + 1) * 1.0 / len(seq),\n",
        "        'user_flair': int(re.sub('[^0-9]', '', entry[FLAIR_INDEX])) if (\n",
        "            re.sub('[^0-9]', '', entry[FLAIR_INDEX]).strip()) else 0,\n",
        "        'authentication': 1 if entry[AUTHOR_ID_INDEX] == cur_thread[TH_AUTHOR_INDEX] else 0,\n",
        "\n",
        "        # ---------------- interaction-based features ----------------\n",
        "        # similarities with the initial post\n",
        "        'simi_initial':simi_init,\n",
        "        # similarity with the recent opinion holder's post  \n",
        "        'simi_recent': cos[i+1].T[0:holder_post_cnt].mean() if holder_post_cnt > 0 \\\n",
        "        and i >= holder_post_cnt \n",
        "        else 0,            \n",
        "        'direct_response': (comment_id_list.index(entry[REPLY_TO_INDEX])+1.0)/len(comment_id_list) if entry[REPLY_TO_INDEX] in comment_id_list\n",
        "        else 0,\n",
        "        'quotation': 1 if \"&gt;\" in entry[CONTENT_INDEX] else 0 ,\n",
        "        'most_neg_simi': most_neg_simi,\n",
        "        'most_pos_simi': most_pos_simi    \n",
        "        }\n",
        "        features_list.append(feature_dict)\n",
        "  \n",
        "    return features_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTtRrwlrcy66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# for skip-chain CRF and edge feature CRF\n",
        "# ------------------------------------------------------------------\n",
        "# Extract features\n",
        "\n",
        "# for each thread, form a feature vector\n",
        "def form_features_vec(seq,cos):\n",
        "    features = []\n",
        "    feature_unioned = Pipeline([\n",
        "        ('extractColumns', ColumnExtractor()),\n",
        "        # Use FeatureUnion to combine the features from subject and body\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list=[\n",
        "                ('seq_stats', Pipeline([\n",
        "                    ('select', ItemSelector(key='seq')),\n",
        "                    ('stats', ProduceStatFeatureDict(cos=cos)),  # returns a list of dicts\n",
        "                    ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
        "                ])),\n",
        "            ],\n",
        "            transformer_weights={\n",
        "                'seq_stats': 1,\n",
        "            },\n",
        "        )\n",
        "         )\n",
        "    ])\n",
        "\n",
        "    output = feature_unioned.fit_transform(seq)\n",
        "\n",
        "    return output.toarray()\n",
        "\n",
        "\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key]\n",
        "\n",
        "\n",
        "class ColumnExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, seq):\n",
        "        column = np.recarray(shape=(len(seq),),\n",
        "                               dtype=[('seq', object), ('content', object)])\n",
        "\n",
        "        column['seq'] = seq\n",
        "        seq_array = np.array(seq)\n",
        "\n",
        "        column['content'] = seq_array[:, CONTENT_INDEX]\n",
        "\n",
        "        return column\n",
        "\n",
        "\n",
        "class ProduceStatFeatureDict(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, cos):\n",
        "        self.cos = cos\n",
        "    \n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, seq):\n",
        "        # get recent posts from the opinion holder\n",
        "        recent_posts = []\n",
        "        cur_thread = threads_dict[seq[0][THREAD_ID_INDEX]]\n",
        "        \n",
        "        # get recent posts from the opinion holder\n",
        "        features_list = []\n",
        "        for i, entry in enumerate(seq):\n",
        "            word_cnt_entry = len(re.split(r'[^0-9A-Za-z]+',entry[CONTENT_INDEX]))\n",
        "            word_token = re.split(r'[^0-9A-Za-z]+',entry[CONTENT_INDEX].lower())\n",
        "\n",
        "            if FEED_PAIRED:\n",
        "                simi_init = self.cos[0][i+1]\n",
        "#                 simi_recent = 0\n",
        "            else:\n",
        "                simi_init = self.cos[0][i+1]\n",
        "\n",
        "            feature_dict = {\n",
        "            # ---------------- linguistic features ----------------\n",
        "            'num_words': word_cnt_entry,\n",
        "            'length_comment': len(entry[CONTENT_INDEX]),\n",
        "            'num_sentences': entry[CONTENT_INDEX].count('.'),  \n",
        "            'hedge': int(entry[HEDGE_INDEX])*1.0/word_cnt_entry,\n",
        "              # from the 2013 ACL - Bias paper\n",
        "            'assertives': int(entry[ASSERTIVES_INDEX])*1.0/word_cnt_entry,\n",
        "            'factives': int(entry[FACTIVES_INDEX])*1.0/word_cnt_entry,\n",
        "            'implicatives': int(entry[IMPLICATIVES_INDEX])*1.0/word_cnt_entry,\n",
        "            'report_verbs': int(entry[REPORT_VERBS_INDEX])*1.0/word_cnt_entry,\n",
        "            'opinion_neg': int(entry[OPINION_NEG_INDEX])*1.0/word_cnt_entry,\n",
        "            'opinion_pos': int(entry[OPINION_POS_INDEX])*1.0/word_cnt_entry, \n",
        "            'strong_subj': int(entry[STRONG_SUBJ_INDEX])*1.0/word_cnt_entry,\n",
        "            'weak_subj': int(entry[WEAK_SUBJ_INDEX])*1.0/word_cnt_entry,\n",
        "             # word category features from [Tan2016WWW]\n",
        "            'definite_articles': entry[CONTENT_INDEX].lower().count('the')*1.0/word_cnt_entry,\n",
        "            'indefinite_articles': sum(entry[CONTENT_INDEX].lower().count(x) for x in (\"a\", \"an\"))*1.0/word_cnt_entry,        \n",
        "            \"1st_pron\": FirstPersonPronUsage(word_token)*1.0/word_cnt_entry,\n",
        "            \"2nd_pron\": SecondPersonPronUsage(word_token)*1.0/word_cnt_entry,\n",
        "            'sentiment': float(re.sub('[^0-9]', '', entry[SENTIMENT_INDEX])) if (\n",
        "                re.sub('[^0-9]', '', entry[SENTIMENT_INDEX]).strip()) else 0,\n",
        "            'num_questions': entry[CONTENT_INDEX].count('?'),     \n",
        "            'examples':  sum(entry[CONTENT_INDEX].lower().count(x) for x in (\"for example\", \"for instance\", \"e.g\", \"eg\")),\n",
        "            'links': 1 if \"http\" in entry[CONTENT_INDEX] else 0,\n",
        "\n",
        "            # ---------------- context-based features ----------------\n",
        "            'order': i + 1,\n",
        "            'length_discussion': len(seq),\n",
        "            'relative_order': (i + 1) * 1.0 / len(seq),\n",
        "            'user_flair': int(re.sub('[^0-9]', '', entry[FLAIR_INDEX])) if (\n",
        "                re.sub('[^0-9]', '', entry[FLAIR_INDEX]).strip()) else 0,\n",
        "            'authentication': 1 if entry[AUTHOR_ID_INDEX] == cur_thread[TH_AUTHOR_INDEX] else 0,\n",
        "            # ---------------- interaction-based features ----------------\n",
        "             # similarities with the initial post\n",
        "            'simi_initial':simi_init,\n",
        "            'quotation': 1 if \"&gt;\" in entry[CONTENT_INDEX] else 0  \n",
        "            }\n",
        "            features_list.append(feature_dict)\n",
        "  \n",
        "        return features_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H21YHZxQfbUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ------------------------------------------------------------------\n",
        "# for edge feature CRF thread structure\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def form_edge_features(features, edges):\n",
        "#     print(\"-------- form_edge_features, node feature --------\")\n",
        "#     print(features[0])\n",
        "#     print(\"-------- form_edge_features, edges --------\")\n",
        "#     print(edges)\n",
        "\n",
        "    edge_features = []\n",
        "    # each edge feature is a numpy array\n",
        "    for pair in edges:\n",
        "        node1 = pair[0] # index of the nodes in features\n",
        "        node2 = pair[1]\n",
        "        edge_feature = []\n",
        "        edge_feature.append(features[node1][0])\n",
        "        edge_feature.append(features[node1][22])\n",
        "        edge_feature.append(features[node1][24])\n",
        "        \n",
        "        edge_feature_vec = np.array(edge_feature)\n",
        "        edge_features.append(edge_feature_vec)\n",
        "    \n",
        "#     print(edge_features)\n",
        "#     print(np.array(edge_features))\n",
        "    return np.array(edge_features)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De_Pq1Czgr6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def calculate_similarity(vec1,vec2):\n",
        "#     cos[i,j] = dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
        "    simi = 1 - spatial.distance.cosine(vec1, vec2) \n",
        "    return simi\n",
        "    \n",
        "\n",
        "def sent_encode(text):\n",
        "    return nltk.sent_tokenize(text)\n",
        " \n",
        "\n",
        "# def USE_simi(docs,cur_thread,thread_op_encoded):\n",
        "def USE_simi(seq,docs,cur_thread):\n",
        "    sentences = []\n",
        "    encoded_sentences = []\n",
        "    thread_author = cur_thread[TH_AUTHOR_INDEX]    \n",
        "\n",
        "    # encode each comments, including texts from opinion holder. docs could be seq or subseq\n",
        "    for i,para in enumerate(docs):\n",
        "        sent_token = nltk.sent_tokenize(para)\n",
        "        sentences.append(sent_token)\n",
        "        if para == \"\":\n",
        "            encoded_sentences.append(np.zeros(len(encoded_sentences[0])))\n",
        "        else:\n",
        "            encoded_sentences.append(encoder.encode(sent_token))\n",
        "        \n",
        "    if len(encoded_sentences) == 0:\n",
        "        print(\"Ah!! empty docs\")\n",
        "        \n",
        "\n",
        "    cos_simi = np.zeros((len(encoded_sentences),len(encoded_sentences)))\n",
        "    \n",
        "    if FEED_PAIRED:\n",
        "        # only need to calculate cos with op\n",
        "        comm_sent_i = encoded_sentences[0]\n",
        "        for j in range(1,len(encoded_sentences)):\n",
        "            comm_sent_j = encoded_sentences[j]\n",
        "            max_simi = 0\n",
        "            for x,y in itertools.product(comm_sent_i,comm_sent_j):\n",
        "                simi = calculate_similarity(x,y)\n",
        "                if simi>max_simi and simi != 1:\n",
        "                    max_simi = simi                \n",
        "            cos_simi[0][j] = max_simi\n",
        "\n",
        "    else:\n",
        "        # calculate cos_simi\n",
        "        for i,comm_sent_i in enumerate(encoded_sentences):\n",
        "            for j in range(i,len(encoded_sentences)):\n",
        "                comm_sent_j = encoded_sentences[j]\n",
        "                # in pairwise, compare sentences in comment i and comment j,\n",
        "                # calculate max(similarity)            \n",
        "                if i==j:\n",
        "                    cos_simi[i][j] = 1\n",
        "                elif seq[i-1][AUTHOR_ID_INDEX] != thread_author and seq[j-1][AUTHOR_ID_INDEX] != thread_author:\n",
        "                    cos_simi[i][j] = 0\n",
        "                    cos_simi[j][i] = 0\n",
        "                else:\n",
        "                    max_simi = 0\n",
        "                    for x,y in itertools.product(comm_sent_i,comm_sent_j):\n",
        "                        simi = calculate_similarity(x,y)\n",
        "                        if simi>max_simi and simi != 1:\n",
        "                            max_simi = simi                \n",
        "                    cos_simi[i][j] = max_simi\n",
        "                    cos_simi[j][i] = max_simi\n",
        "\n",
        "    return cos_simi,sentences \n",
        "\n",
        "print_time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-8q0s3ngwzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initiate an encoder\n",
        "encoder = USE()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA2t3QBOiAB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ProcessThreadSeq(thread_seq, oh_reply_dict):\n",
        "    global cnt_thread, encoder\n",
        "    cnt_thread += 1\n",
        "    cur_thread = threads_dict[thread_seq[0][THREAD_ID_INDEX]]\n",
        "    thread_author = cur_thread[TH_AUTHOR_INDEX]\n",
        "    prev_author = \"\"\n",
        "    sub_seq = []\n",
        "    sent_token_seq = []\n",
        "       \n",
        "    # ----------------------------------------------------------------------------\n",
        "\n",
        "    for i, row in enumerate(thread_seq):\n",
        "        sent_token = nltk.sent_tokenize(row[CONTENT_INDEX].lower())\n",
        "        # form sub sequences for each thread: \n",
        "        if prev_author != \"\" and (prev_author != thread_author and \\\n",
        "                                  row[AUTHOR_ID_INDEX] == thread_author and len(sub_seq) >= 3):\n",
        "            ProcessSubSeq(sub_seq,cur_thread, oh_reply_dict)\n",
        "#             sent_token_seq = []\n",
        "            sub_seq = []\n",
        "#             sent_token_seq.append(sent_token)\n",
        "            sub_seq.append(row)\n",
        "        else:\n",
        "            sub_seq.append(row)\n",
        "#             sent_token_seq.append(sent_token)\n",
        "        prev_author = row[AUTHOR_ID_INDEX]\n",
        "        \n",
        "    ProcessSubSeq(sub_seq,cur_thread, oh_reply_dict)\n",
        "    \n",
        "\n",
        "# test 2: mix sub-sequences with/without delta\n",
        "def ProcessSubSeq(seq, cur_thread,oh_reply_dict):\n",
        "    global cnt_ignored_seq, delta_record_cnt, nondelta_record_cnt, delta_threads\n",
        "    global cnt_train_subseq,cnt_test_subseq,data_raw_set\n",
        "\n",
        "    # add the initial post to the beginning of each seq or subseq\n",
        "    thread_op = cur_thread[TH_TITLE_INDEX]+\". \"+cur_thread[TH_TEXT_INDEX] if len(cur_thread[TH_TEXT_INDEX]) > 0  else cur_thread[TH_TITLE_INDEX]\n",
        "    seq_array = np.array(seq)\n",
        "    docs = [thread_op] + seq_array[:, CONTENT_INDEX].tolist()\n",
        "    \n",
        "    simi = np.zeros((len(docs),len(docs)))\n",
        "    simi,sent_seq = USE_simi(seq,docs,cur_thread)\n",
        "    # --------------------------------\n",
        "\n",
        "    features = form_feature_dicts(seq, sent_seq, simi, oh_reply_dict)\n",
        "    labels = form_labels(seq)\n",
        "\n",
        "    delta_sum = sum(list(map(int, labels)))\n",
        "        \n",
        "        \n",
        "    if len(features[0]) < FEATURE_NUM:\n",
        "        print(seq[0][0] + \" - less features\")\n",
        "        cnt_ignored_seq += 1\n",
        "    elif HAS_DELTA_ONLY and delta_sum == 0:\n",
        "        cnt_ignored_seq += 1\n",
        "    else:\n",
        "        data_set.append(features)\n",
        "        data_label.append(labels)\n",
        "        \n",
        "        data_raw_set.append(seq)\n",
        "\n",
        "\n",
        "\n",
        "print(\"complete:\")\n",
        "print_time()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkBNvEFIAtFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GetTokenAndCos(thread_seq):\n",
        "    cur_thread = threads_dict[thread_seq[0][THREAD_ID_INDEX]]\n",
        "    thread_author = cur_thread[TH_AUTHOR_INDEX]\n",
        "    \n",
        "    encoded_thread = {}\n",
        "    \n",
        "    # calculate similarity between comments based on the whole thread\n",
        "    \n",
        "    # --------------------------------\n",
        "    # option 4 - calculate cos with USE at sentence level and calculate the max(similarity)\n",
        "    \n",
        "    # add the initial post to the beginning of each seq or subseq\n",
        "    thread_op = cur_thread[TH_TITLE_INDEX]+\". \"+cur_thread[TH_TEXT_INDEX] if len(cur_thread[TH_TEXT_INDEX]) > 0  else cur_thread[TH_TITLE_INDEX]\n",
        "    comments =  np.array(thread_seq)\n",
        "    docs = comments[:, CONTENT_INDEX]\n",
        "    \n",
        "    thread_id = thread_seq[0][THREAD_ID_INDEX]    \n",
        "    \n",
        "    encoded_thread[thread_id] = USE_simi_preprocess(thread_op,docs,cur_thread)\n",
        "    \n",
        "#     print(encoded_thread)\n",
        "    \n",
        "    return encoded_thread\n",
        "\n",
        "\n",
        "# def USE_simi(docs,cur_thread,thread_op_encoded):\n",
        "def USE_simi_preprocess(thread_op,docs,cur_thread):\n",
        "    sentences = []\n",
        "    encoded_sentences = []\n",
        "    thread_author = cur_thread[TH_AUTHOR_INDEX]    \n",
        "    \n",
        "    # encode each comments, including texts from opinion holder. docs could be seq or subseq\n",
        "    for i,para in enumerate(docs):\n",
        "        sent_token = nltk.sent_tokenize(para)\n",
        "        sentences.append(sent_token)\n",
        "        if para == \"\":\n",
        "            encoded_sentences.append(np.zeros(len(encoded_sentences[0])))\n",
        "        else:\n",
        "            encoded_sentences.append(encoder.encode(sent_token))\n",
        "        \n",
        "    if len(encoded_sentences) == 0:\n",
        "        print(\"Ah!! empty docs\")\n",
        "        \n",
        "\n",
        "    # simi with op    \n",
        "    simi_op = np.zeros(len(docs))\n",
        "    if thread_op != \"\":\n",
        "        token_op = nltk.sent_tokenize(thread_op)\n",
        "        encoded_op = encoder.encode(token_op)\n",
        "\n",
        "    for j in range(len(encoded_sentences)):\n",
        "        comm_sent_j = encoded_sentences[j]\n",
        "        max_simi = 0\n",
        "        for x,y in itertools.product(encoded_op,comm_sent_j):\n",
        "            simi = calculate_similarity(x,y)\n",
        "            if simi>max_simi and simi != 1:\n",
        "                max_simi = simi                \n",
        "        simi_op[j] = max_simi\n",
        "    \n",
        "    # simi for paired comments\n",
        "    simi_comments = np.zeros((len(encoded_sentences),len(encoded_sentences)))\n",
        "    \n",
        "    for i,comm_sent_i in enumerate(encoded_sentences):\n",
        "        for j in range(i,len(encoded_sentences)):\n",
        "            comm_sent_j = encoded_sentences[j]\n",
        "            # in pairwise, compare sentences in comment i and comment j,\n",
        "            # calculate max(similarity)            \n",
        "            if i==j:\n",
        "                simi_comments[i][j] = 1\n",
        "            else:\n",
        "                max_simi = 0\n",
        "                for x,y in itertools.product(comm_sent_i,comm_sent_j):\n",
        "                    simi = calculate_similarity(x,y)\n",
        "                    if simi>max_simi and simi != 1:\n",
        "                        max_simi = simi                \n",
        "                simi_comments[i][j] = max_simi\n",
        "                simi_comments[j][i] = max_simi\n",
        "\n",
        "    return (simi_op,simi_comments,sentences, encoded_sentences)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o7YAuTYiOLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Linear CRF\n",
        "\n",
        "comments_reader = csv.reader(open(\"data_cmv/cmv_comments.csv\", newline='', encoding='mac_roman'), delimiter=',',\n",
        "                        quotechar='\"')\n",
        "\n",
        "prev_thread = \"\"\n",
        "# prev_author = \"\"\n",
        "thread_seq = []\n",
        "# thread_content_seq = []\n",
        "\n",
        "oh_reply_dict = {} # key is the comment id which OP replied to; value is tuple (text, sentiment, order_index)\n",
        "\n",
        "# variables\n",
        "data_set = []\n",
        "data_label = []\n",
        "\n",
        "train_set = []\n",
        "train_label = []\n",
        "\n",
        "test_set = []\n",
        "test_label = []\n",
        "\n",
        "data_raw_set = []\n",
        "encoded_all_comments = []\n",
        "\n",
        "try: \n",
        "    print_time()\n",
        "    row_cnt = 0\n",
        "    \n",
        "    for row in comments_reader:\n",
        "        row_cnt += 1\n",
        "        if len(row) == 0 or row_cnt == 1:\n",
        "            continue\n",
        "            \n",
        "        # form a sequence for each thread        \n",
        "        cur_thread = threads_dict[row[THREAD_ID_INDEX]]\n",
        "        thread_author = cur_thread[TH_AUTHOR_INDEX]\n",
        "        \n",
        "        if row[AUTHOR_ID_INDEX] == thread_author:\n",
        "            oh_reply_dict[row[REPLY_TO_INDEX]] = (row[CONTENT_INDEX], row[SENTIMENT_INDEX], row[COMMENT_ORDER_INDEX])\n",
        "        \n",
        "        if row[THREAD_ID_INDEX] == prev_thread or prev_thread == \"\":\n",
        "            thread_seq.append(row)\n",
        "\n",
        "        elif prev_thread != \"\" and prev_thread != \"thread_id\":\n",
        "            ProcessThreadSeq(thread_seq, oh_reply_dict)\n",
        "            thread_seq = []\n",
        "            oh_reply_dict = {}\n",
        "            thread_seq.append(row)\n",
        " \n",
        "    \n",
        " \n",
        "        prev_thread = row[THREAD_ID_INDEX]\n",
        "\n",
        "        if row_cnt % 10000 == 0:\n",
        "            print(row_cnt, datetime.datetime.now())\n",
        "            \n",
        "    # for the last thread\n",
        "    print(row_cnt, oh_reply_dict)\n",
        "    ProcessThreadSeq(thread_seq, oh_reply_dict)\n",
        "    print(\"Done processing.\")\n",
        "\n",
        "except Exception as ex:\n",
        "    sys.stderr.write('Exception\\n')\n",
        "    extype, exvalue, extrace = sys.exc_info()\n",
        "    traceback.print_exception(extype, exvalue, extrace)\n",
        "    #sys.exc_clear()\n",
        "\n",
        "print_time()\n",
        "# print(row)\n",
        "print(\"total encoded dataset: \", len(encoded_all_comments)) # show number of encoded threads\n",
        "print(\"total raw data: \", len(data_raw_set))\n",
        "\n",
        "# print(data_raw_set[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4_nV3xbQBkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# process sequences from pickled data for edge CRF\n",
        "# transform it to linear-crf structure\n",
        "\n",
        "TRAIN_THREAD_NUM = 5000 #10000 comments: 227 threads; 1000 comments: 13 threads\n",
        "\n",
        "data_set_loaded = pickle_load(\"data_cmv/pkl/edge_thread_dataset.pkl\")\n",
        "\n",
        "# split the dataset into train and test\n",
        "train_set = data_set_loaded[\"train_set\"]\n",
        "train_label = data_set_loaded[\"train_label\"]\n",
        "\n",
        "test_set = data_set_loaded[\"test_set\"]\n",
        "test_label = data_set_loaded[\"test_label\"]\n",
        "\n",
        "# combine the incorrectly-splitted train-test into dataset\n",
        "data_set = train_set + test_set\n",
        "data_label = train_label + test_label\n",
        "\n",
        "\n",
        "print(len(data_set), len(data_label))\n",
        "\n",
        "print_time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuLrMyMZ7bD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs', \n",
        "    c1=0.1, \n",
        "    c2=0.1, \n",
        "    max_iterations=100, \n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "print_time(\"Start fitting...\")\n",
        "\n",
        "crf.fit(train_set, train_label)\n",
        "\n",
        "print_time(\"Done fitting.\")\n",
        "\n",
        "# pickle trained crf\n",
        "if pickle_overwrite:\n",
        "    fileObject = open(file_model+\"_\"+curtime()+\".pkl\",'wb') \n",
        "    pickle.dump(crf,fileObject)\n",
        "    fileObject.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjRTewb37f-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_overwrite = True    \n",
        "\n",
        "# Return double of n \n",
        "def pos_value(lst): \n",
        "    return [ele['1'] for ele in lst] \n",
        "\n",
        "labels = list(crf.classes_)\n",
        "\n",
        "# ------------------------------\n",
        "# marginal probability of label\n",
        "\n",
        "predict_result_bi = crf.predict(test_set)\n",
        "predict_result = crf.predict_marginals(test_set) # y_score for both classes '0' and '1'\n",
        "\n",
        "# pickle predicted results\n",
        "if pickle_overwrite:\n",
        "    fileObject = open(\"test_linear_seq_new_result_bi_\"+curtime()+\".pkl\",'wb') \n",
        "    pickle.dump(predict_result_bi,fileObject)\n",
        "    fileObject.close()\n",
        "\n",
        "    fileObject = open(\"test_linear_seq_new_result_\"+\"_\"+curtime()+\".pkl\",'wb') \n",
        "    pickle.dump(predict_result,fileObject)\n",
        "    fileObject.close()\n",
        "\n",
        "y_score = list(map(pos_value, (lst for lst in predict_result))) \n",
        "\n",
        "test_label_trans = list(map(int,np.hstack(test_label)))\n",
        "predict_label_trans = list(map(float,np.hstack(y_score)))\n",
        "\n",
        "print_time()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI7ztSw47lg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# AUC-ROC plot\n",
        "\n",
        "print('ROC-AUC: ', roc_auc_score(test_label_trans, predict_label_trans))\n",
        "fpr, tpr, thresholds = roc_curve(test_label_trans, predict_label_trans, pos_label=1)\n",
        "\n",
        "print(classification_report(np.concatenate(test_label), np.concatenate(predict_result_bi)))\n",
        "\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "import matplotlib\n",
        "# matplotlib.use(\"TkAgg\")\n",
        "matplotlib.rcParams.update({'font.size': 20})\n",
        "f=plt.figure()\n",
        "plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()\n",
        "if pickle_overwrite:\n",
        "    f.savefig(\"data_cmv/figures/exp-\"+test_name+\"-auc-\"+curtime()+\".pdf\", bbox_inches='tight')\n",
        "\n",
        "print_time()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9X5lfzn7mH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# precision-recall-curve (PRC)\n",
        "\n",
        "test_label_trans = list(map(int,np.hstack(test_label)))\n",
        "predict_label_trans = list(map(float,np.hstack(y_score)))\n",
        "\n",
        "print(test_label_trans[:10])\n",
        "print(predict_label_trans[:10])\n",
        "\n",
        "average_precision = average_precision_score(test_label_trans, predict_label_trans)\n",
        "\n",
        "print('PRC for skip-chain CRF with sub-sequences \\n {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "# plot PRC\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(test_label_trans, predict_label_trans)\n",
        "\n",
        "pos_rate = sum(test_label_trans)/len(predict_label_trans)\n",
        "\n",
        "f=plt.figure()\n",
        "matplotlib.rcParams.update({'font.size': 20})\n",
        "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.step(recall, precision, color='b', alpha=0.2,\n",
        "         where='post', label='AP = %0.2f' % average_precision)\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "plt.plot([0, 1], [pos_rate, pos_rate], 'r--')\n",
        "plt.legend(loc='upper right')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "if pickle_overwrite:\n",
        "    f.savefig(\"data_cmv/figures/exp-\"+test_name+\"-prc-\"+curtime()+\".pdf\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "print_time()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ucfk2Oq78_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# measure MRR (mean reciprocal rank)\n",
        "# wikipedia: https://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
        "\n",
        "print(\"sub seq total #\", len(y_score))\n",
        "ranked_results = []\n",
        "\n",
        "for i in range(len(y_score)):\n",
        "    list1 = test_label[i]\n",
        "    list2 = y_score[i]\n",
        "    list2, list1 = zip(*sorted(zip(list2,list1), reverse=True))\n",
        "    list1_ranked = list(map(int, list1))\n",
        "    # ignore non-delta sub sequences\n",
        "    if sum(list1_ranked) != 0:\n",
        "        ranked_results.append(list1_ranked)\n",
        "\n",
        "print(\"sub seq for mrr #\", len(ranked_results))\n",
        "\n",
        "rs = (np.asarray(r).nonzero()[0] for r in ranked_results)\n",
        "\n",
        "mrr = np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
        "\n",
        "print(mrr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}